# Sum-Product Networks: what, why, and how you can use them too.

    Are you interested in learning about a class of model that can perform efficient exact inference? In this post, we will:  
        1. Explore the concept of sum-product networks  
        2. Provide a detailed overview of their structure and operations  
        3. Explain theoretical principles that underpin the effectiveness of sum-product networks  
        4. Discuss various applications and extensions of sum-product networks which highlight their effectiveness.

    If you'd like to try it out for yourself, the demo portion of the post outlines basic operations using an open-source library. Finally, we will delve into comparisons between sum-product networks and other models to show off their unique properties. Whether you are a machine learning practitioner looking to understand the capabilities of sum-product networks, or a researcher interested in the developments of this exciting field, this blog post is for you. So, let's dive in and learn about sum-product networks. 

## What are Sum-Product Networks?

     To begin, we will define sum-product networks in relation to reasoning. **Sum-product networks (SPNs)** are a subcategory of mathematical models that have a wide variety of applications on complex systems, mainly probabilistic reasoning and other Bayesian inference. Probabilistic inference is the process of determining the probability of certain events occurring on the basis of observed data.  Graphical models represent the relationships between random variables, and allow the calculation of different event likelihoods from probability distributions. Sum-product networks get their name from their structure, a combination of sum and product nodes. Sum nodes represent the "sum" operation and allow the SPN to capture the inherent uncertainty or randomness in the data. Product nodes, on the other hand, represent the "product" operation and capture the dependencies between different random variables. 

SPN Structure Example<sup>3</sup>

![](https://camo.githubusercontent.com/e721facf788f07798f5031432b823b16925eebfacbc6b7d78b57b7c4629aa254/68747470733a2f2f6c68362e676f6f676c6575736572636f6e74656e742e636f6d2f58326a326945636b79377832737966494b796570496f796e335a4a67494147576d305830425871636d6874376e703558626c474275766e696e7a75384d4d515873754f415f5a726b544e5436417751443646725f5f7348676d376453455371494468726a2d5f6d445555644b643938597950336d457656634f675a3676447079566e6a37445f384162515f3362485764755532634c6376413241613153395a486b5642302d352d74537a6362645364584d647553454e61575269654f6541)

    By combining these two types of nodes, an SPN can be used to compactly represent a complex probability distribution and perform efficient inference on that distribution. The edges of the network encode the relationships between the variables, through the usage of a directed acyclic graph to represent the functions. The output of a sum-product network is computed by performing a series of multiplications and weighted sums, following the structure of the graph. Encoding the operations facilitates the efficient computation of the probability of different events occurring in the system with linear complexity, making sum-product networks a powerful tool for rapid reasoning under uncertainty. SPN’s have key properties that allow for efficient inference, including decomposability, consistency, validity, and completeness.

_SPN Properties_

<table><tbody><tr><td>Property</td><td>Function</td></tr><tr><td>Decomposability</td><td>Allows SPNs to be divided into a set of sub-SPNs, each of which are individually valid.&nbsp;</td></tr><tr><td>Consistency</td><td>Ensures that the probabilities generated by the SPN match the true probabilities of the distribution variables. According to Poon &amp; Domingos3, consistency is encompassed by decomposability.</td></tr><tr><td>Validity</td><td>Ensures that the probability distribution defined by the SPN is accurately represented by the SPN. Formally, a valid SPN is one where the probabilities associated with each sum nodes' edge weights are non-negative, and sum up to 1. A valid SPN is complete.</td></tr><tr><td>Completeness</td><td>An SPN is complete if it can represent any distribution over the modeled variables. This is done by representing the joint probability distribution as a product of individual conditional distributions.&nbsp;</td></tr></tbody></table>

## SPN Extensions and Applications

    Although SPNs are already powerful enough to support efficient exact inference, there are extensions to support more complex inference. Dynamic SPNs (DSPNs) represent models that change over time, and are well suited for temporal dependencies between variables. They are particularly useful when multiple processes interact, or when latent variables are not directly observed. Sum-Product Max Networks (SPMNs) compute the values of variables that maximize the posterior probability, given evidence, using maximum a posteriori (MAP) estimates. SPMNs are able to perform probabilistic and MAP inference, to extend SPNs to decision making. There are various flavors implemented to these extensions, such as Recurrent SPNs (RSPNs) and Recurrent SPMNs (RSPMNs). RSPNs combine recurrent neural networks with SPNs, and are suited for variables that are dependent on the previous variables in the sequence. The recurrent aspect utilizes neural networks, allowing feedback connections to model dependencies between variables by computing hidden states at each time step.  

    SPNs arsenal of extensions makes them indispensable, and applicable to a wide array of tasks. A common application of SPNs is in natural language processing by modeling the probability of word occurrences. This is useful in tasks related to speech recognition and machine translation, particularly when the goal is to determine the most likely sequence of words. This application may be extended to automatically generate text that is similar to an input. SPNs may also be used in computer vision in modeling the probability of different objects appearing in an image. This application can help in image segmentation, in which the goal is to identify and classify different image objects, or object detection, where the goal is to locate and identify image objects. More so, SPNs can be employed in computer vision to automatically generate novel images that are related to the input. In regard to artificial intelligence, SPNs can be deployed in planning and decision making. Through modeling the probabilities of various events occurring as a result of observed data, an optimal series of an agent’s actions can be determined to achieve an overarching goal. Other applications of SPNs include medical diagnosis, fraud detection, bioinformatics, and other pattern recognition and probabilistic domains.

## Interested? Try it out yourself with SPFlow:

   Undoubtedly, there is at least one aspect in every probabilistic domain that can be addressed with SPNs, or one of its many extensions. If you are interested in using SPNs in your domain, SPFlow is an open-source Python library for developing SPNs. Tools are provided for defining and training network structure, probability distributions, and even optimization algorithms for model parameters. After generating an SPN, SPFlow also provides functions for performing probabilistic inferences. These inferences include computing marginalization, log-likelihoods, conditional sampling, and classification through most probable explanation (MPE). To get started with SPFlow, here are some code blocks from the SPFlow documentation for basic SPN creation and inference<sup>4</sup>. 

    Using DSL, an SPN with categorical leaves can be defined as follows:

```python
from spn.structure.leaves.parametric.Parametric import Categorical
spn = 0.4 * (Categorical(p=[0.2, 0.8], scope=0) *
             (0.3 * (Categorical(p=[0.3, 0.7], scope=1) *
                     Categorical(p=[0.4, 0.6], scope=2))
            + 0.7 * (Categorical(p=[0.5, 0.5], scope=1) *
                     Categorical(p=[0.6, 0.4], scope=2))))
    + 0.6 * (Categorical(p=[0.2, 0.8], scope=0) *
             Categorical(p=[0.3, 0.7], scope=1) *
             Categorical(p=[0.4, 0.6], scope=2))
```

    Using object hierarchy, an SPN with categorical leaves can be defined as follows:

```python
from spn.structure.leaves.parametric.Parametric import Categorical
spn = 0.4 * (Categorical(p=[0.2, 0.8], scope=0) *
             (0.3 * (Categorical(p=[0.3, 0.7], scope=1) *
                     Categorical(p=[0.4, 0.6], scope=2))
            + 0.7 * (Categorical(p=[0.5, 0.5], scope=1) *
                     Categorical(p=[0.6, 0.4], scope=2))))
    + 0.6 * (Categorical(p=[0.2, 0.8], scope=0) *
             Categorical(p=[0.3, 0.7], scope=1) *
             Categorical(p=[0.4, 0.6], scope=2))
```

   Congrats, you now have an SPN! To visualize your newly created SPN, you can use the following function to create and save an image:

```python
from spn.io.Graphics import plot_spn plot_spn(spn, 'basicspn.png')
```

![](https://user-images.githubusercontent.com/103450057/209725801-3c195e38-e0ae-43c9-a767-ed4502f7572b.png)

     To perform inference, here’s an example of marginalizing an SPN to only include variables V1 and V2. To calculate the log likelihood of the marginalized SPN, a numpy array is used.

```python
import numpy as np
from spn.algorithms.Marginalization import marginalize
from spn.algorithms.Inference import log_likelihood

spn_marg = marginalize(spn, [1,2])
test_data = np.array([1.0, 0.0, 1.0]).reshape(-1, 3)

llm = log_likelihood(spn_marg, test_data)
print('Marginalized LL')
print(llm, np.exp(llm))

ll = log_likelihood(spn, test_data)
print('Basic SPN LL')
print(ll, np.exp(ll))
```

  The output demonstrates the change in log-likelihoods when an SPN is marginalized. 

```python
Marginalized LL
[[-1.68416146]] [[0.1856]]
Basic SPN LL
[[-1.90730501]] [[0.14848]]
```

   SPFlow is extensible, supporting SPMNs, RSPNs, MSPNs, and more. If you would like a fully in depth tutorial, [here's the documentation](https://spflow.github.io/SPFlow/index.html) you'll need as well as the [most up to date GitHub page](https://github.com/SPFlow/SPFlow). 

## **Sum Product Networks vs. Other Models**

    Now that we are familiar with the workings of SPNs, let's discuss how they compare to two other machine learning models: arithmetic circuits and neural networks. We will be comparing the models in terms of their capabilities, differences, and potential applications. 

### _Arithmetic Circuits vs Sum-Product Networks_

**Arithmetic circuits (ACs)** are another graphical model used for representing and computing functions in a way that is both efficient and mathematically rigorous. These models are similar in that they both use a graphical structure to represent a mathematical function, and the nodes in the graph correspond to operations that are performed on the inputs to the function. There are a few key differences between SPNs and ACs. SPNs are a type of probabilistic graphical model, meaning that they are used in representing probability distributions over a set of random variables. Arithmetic circuits are deterministic models, insofar that they always compute the same output for a given set of inputs. A key difference between the models is that SPNs are restricted to sum and product operations as the basic building blocks of computation, while ACs may use a more general set of operations including multiplication and addition. As a result of this, ACs can be more expressive in their computation of a wider range of functions in comparison to SPNs. 

    While SPNs and ACs are similar, they are applied for different purposes and have some differences in their mathematical properties and expressive power. Both SPNs and ACs are used in a variety of applications, such as machine learning and computational complexity theory. SPNs are generally considered to be more efficient than ACs, because they can exploit the structure of the function being computed in order to reduce the computational complexity. Arithmetic circuits are used for performing complex mathematical computations, while sum-product networks are used for performing probabilistic inference and other types of statistical computations.

Logic Diagram of an Example Arithmetic Circuit<sup>2</sup>

![](https://user-images.githubusercontent.com/103450057/209029925-57ee077d-501b-4f2b-9a34-1f1637f141c3.png)

    Due to AC’s ability to compute complex mathematical computations, they have a wide range of applications. In cryptography, the computations done by AC’s are challenging to reverse, lending themselves to the encryption and decryption of sensitive information. AC’s can also be used in data compression and algebraic geometry, as they have the ability to solve equations with many variables, and compress large datasets through representation via mathematical operations. One major application of AC’s is machine learning, as they can be used to train neural networks due to their efficient handling of large data sets. Another structure AC’s can be deployed in are support vector machines, as they require complex optimization which may be computed through the use of AC’s.

### _Neural Networks vs Sum-Product Networks_

**Neural networks (NNs)** are a form of machine models that predict outcomes based on input data. One difference between SPNs and NNs is their structures, with the former utilizing graphs while the latter implements a layered architecture of interconnected nodes. SPNs utilize a structure learning algorithm that uncovers the relationships between variables without any prior knowledge. This is beneficial in applications where probabilistic reasoning is the main priority. Human brain structure and function inspire the structure of the NN model, which is composed of interconnected nodes used to represent data attributes. NNs make use of a combination of linear, and nonlinear, operations to compute predictions and represent the relationship between inputs and outputs. The operations are a wide variety of mathematical operations, including dot products and non-linear activation functions, to learn and transform the input data. 

   While SPNs are typically used for probabilistic inference, NNs are deployed in machine learning algorithms like classification, regression, and clustering.  Concerning explainability, SPNs are more interpretable and easily understood, while NNs are considered to be black box models due to their difficulty in visualizing the reasoning behind outputs. Even with their superior efficiency for specific problems, SPNs are less computationally expensive in comparison to NNs. However, NNs generally outperform SPNs on complex datasets. There are several training algorithms for SPNs that adjust the parameters of the network to maximize the likelihoods of observed data, while NNs employ backpropagation to adjust the connection weights between nodes to make predictions.

Basic Neural Network Structure<sup>1</sup>

![](https://user-images.githubusercontent.com/103450057/209030047-3868ff24-5ae0-4f31-8d75-204180f25665.png)

    Both SPNs and NNs are types of machine learning algorithms, but the differences outlined between them make them more inclined to specific tasks. NNs are particularly well suited for tasks that involve complex non-linear relationships in unstructured data, like image and speech recognition, and have been successfully applied to a wide range of pattern recognition tasks. NNs may be implemented in computer vision to achieve facial recognition and object detection in self-driving cars, predictive modeling for the stock market, and recommendation modeling for advertisements. SPNs are more suited for tasks related to probabilistic inference involving structured data, and can be trained using maximum likelihood estimation and gradient descent. Applications of SPNs include robots in complex environments, data analysis in medical diagnosis and social sciences, and machine learning tasks.

    In summary, SPNs are more focused on representing and reasoning about uncertainty, while neural networks revolve around learning from data and making predictions.  Arithmetic circuits are more general than SPNs, as the wider variety of functions provide higher expressivity. Each computational model can be useful in different contexts due to their underlying mechanisms, and may even be combined to take advantage of their complementary strengths. SPNs, ACs, and NNs are helpful tools in machine learning, but they are designed for different purposes and have varying strengths and weaknesses. 

<table><tbody><tr><td>Model</td><td>Applications</td><td>Strengths and Weaknesses</td></tr><tr><td>Sum-Product Networks</td><td><ul><li>Natural Language Processing</li><li>Computer Vision</li><li>Image Segmentation</li><li>Object Detection/Identification</li><li>Fraud Detection</li><li>Bioinformatics</li></ul></td><td><p>Strengths:</p><ul><li>Linear complexity inference</li><li>Flexible inferencing&nbsp;</li><li>Semantic representation</li></ul><p>Weaknesses:</p><ul><li>Limited expressiveness</li><li>Sensitivity to initialization of variables and weights</li></ul></td></tr><tr><td>Arithmetic Circuits</td><td><ul><li>Cryptography</li><li>Data Compression</li><li>Algebraic Geometry</li><li>Support Vector Machines</li></ul></td><td><p>Strengths:</p><ul><li>Efficient computation</li><li>Flexibility</li><li>Robust to noise</li></ul><p>Weaknesses:</p><ul><li>Lack of interpretability</li><li>Higher computational complexity</li></ul></td></tr><tr><td>Neural Networks</td><td><ul><li>Classification, Regression, Clustering</li><li>Image/Speech Recognition</li><li>Facial Recognition</li><li>Predictive Modeling</li><li>Medical Diagnosis</li></ul></td><td><p>Strengths:</p><ul><li>Versatility in application domain</li><li>High performance and accuracy with complex data sets</li></ul><p>Weaknesses:</p><ul><li>Potential of overfitting</li><li>Long-term dependencies</li><li>Black box</li></ul></td></tr></tbody></table>

_References:_

1.  Shukla, Lavanya. “Designing Your Neural Networks.” _Medium_, Towards Data Science, 23 Sept. 2019, [https://towardsdatascience.com/designing-your-neural-networks-a5e4617027ed.](https://towardsdatascience.com/designing-your-neural-networks-a5e4617027ed.%C2%A0)
2.  Sarangi, Saumyakanta, et al. “VHDL Implementation of Arithmetic Logic Unit.” _International Journal of Engineering Research & Technology_, vol. 3, no. 4, Apr. 2014. 
3.  Poon, Hoifung, and Pedro Domingos. “Sum-Product Networks: A New Deep Architecture.” _2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)_, 2011, [https://doi.org/10.1109/iccvw.2011.6130310.](https://doi.org/10.1109/iccvw.2011.6130310.%C2%A0)
4.  Molina, Alejandro, et al. “SPFlow: An Easy and Extensible Library for Deep Probabilistic Learning using Sum-Product Networks” 11 Jan. 2019, [https://arxiv.org/abs/1901.03704](https://arxiv.org/abs/1901.03704).
